[{"content":"Continuous learning is a critical skill. The internet houses a panoply of learning material for almost any topic under the sun. YouTube is my go-to for learning anything and everything - for it is easy to digest and entertaining. I\u0026rsquo;d regard YouTube to be on the lower side of information density. Contrastingly, reading acadmic papers is something I\u0026rsquo;ve begun recently. YouTube and academic papers are on the opposite extremes of information density - which is why I like reading papers.\nThrough this post, I share my inclinations to read papers, some frameworks to understand them and how communities like Papers We Love are essential to catalyze essential discourse.\nMy Experience I had my first stint with reading research papers for college. We had a final year project which involved preparing literature surveys and finally producing a paper of our own. This introduced me to looking up and understanding papers. Finally, writing one was a good experience on producing structured technical literature.\nOne of my friends introduced me to 6.824, MIT Distributed Systems course by MIT. This course is taught through dissecting research papers - from the early Map Reduce to Raft and finally large scale distributed systems like Blockchains. One of my biggest takeaways from this course is the exposure to the fascinating world of understanding computer science from a perspecive of academic research - the key aspect being able to grasp the evolutionary nature of scientific development.\nEssentially, science isn\u0026rsquo;t particularly definite. It is an evolutionary body of knowledge and experimentation lies at the heart of it. Each experiment produces a new block of knowledge, thus expanding our sphere of understanding. Similarly, future experiments will build on the knolwedge from past experiments. If we see each paper as an experiment or observation, we will be able to see the chronological development of this body of knowledge. In subtle sense, the paper is a part of a story being told of science\u0026rsquo;s evolution - a paper explains why such an experiment was carried out, how it was carried out, what the observations were, etc.\nCritical Thinking Consequently, while we go over a paper, we may sometimes pause to ponder over the process of experimentation carried out in it, the style of implementation, etc. This opens up an arena for discussions regarding the multitude of factors considered while developing that paper. As a result of this, we subconsciously imbibe the fact that everything brings with it a tradeoff - some subtle and some apparent. This kind of mindset, I believe, is crucial for deepening your understanding of a subject and might possibly open door for innovation.\nInformation Dense in a Hyper-marketed World Critical thinking demands comprehensive knowledge in the subject matter and unfortunately, the currrent world is bombarded with hyper marketed content - to milk out every last cent of profit and to exploit market segments. This is especially apparent in industries like blockchain and finance. In situations like this, a research paper, as it is verified by academically certified panelists, is a boon for gathering information on a topic. And it is upto you to zoom in or out on the details - and this provides you the flexibility to easily understand the topic, verify the information and decide whether to continue in this direction.\nSummary and Implementation After reading a paper, I try to summarize the learnings. Sometiems I take a step further and try to implement aspects of it or in its entirety. This is time demanding but makes the understanding concrete.\nCommunity Previously I had mentioned critical thinking which involves understanding the tradeoffs, rationalizing the choices taken by the author and thiking of ways to improve the paper. While all of this involves only the effort of the self, it is infinitely more pleasureable to discuss this paper with a like minded group of individuals. Pinging off ideas off of each other, trying to understand the subject matter through a different perspective from another perpective, etc is invaluable. This is where communities like PapersWeLove come into the picture. I urge your to explore these.\nFrameworks for Reading Papers Academic papers are structured and almost every paper has a similar pattern. Throughout my journey, I\u0026rsquo;ve discovered a few frameworks for reading papers. These frameworks guide you to easily understanding a paper - whether you are looking for surface level information to decide if you\u0026rsquo;d want to continue down this path or if you want to deep dive into the domain. The Three Step Pass is one of most popular ones I\u0026rsquo;ve found. Below is an outline. The full method is available as a paper here (kinda meta right?)\nThe Three-Pass Approach Follow the three passes, where each pass will conclude with you gaining deeper and deeper knowledge of the subject.\nFirst Pass This is a quick scan which will help you decide whether to do any more passes.\nRead the section headings to get a bird\u0026rsquo;s eye view of the paper. You can glance over the references, mentally ticking off the ones you\u0026rsquo;ve already read. At the end of the first pass, you should be able to anser the 5 C\u0026rsquo;s:\nCategory of paper (measurement, analysis, description, review, etc) Context Correctness Main contributions Clarity: Ss the paper well written Second Pass This step is to understand the key points, so ignore details such as proofs.\nLook carefully at illustrations and correctness - this helps to separate shoddy work. Remember to mark unread references and unknown words - revisit them back to learn about the background of the paper. After this pass, you should be able to summarize the main premise of the paper, with supporting evidence, to someone else.\nThird Pass This is an attempt to virutally re-implement the paper. This stage will help you identify not only the paper\u0026rsquo;s innovations, but also its hidden findings and assumptions.\nIdentify and challenge every assumtption. And think about how you yourself would present a particular idea. Jot down ideas for future work. After this pass, you should be able to reconstruct the entire paper from memory - pinpoint implicit assumptions, missing citations to relevant work, potential issues with experimental or analytical techniques.\nConclusion In conclusion, while reading papers is a pleasure, it is time consuming. A lot of information is available in more concise forms - some in enjoyable video formats. Reading papers isn\u0026rsquo;t a necessary skill but an experience on its own.\nIf you want to go down this path, here is a convincing article on reading academic papers by the founding members of the PapersWeLove team: You should be reading academic computer science papers\n","permalink":"https://123vivekr.github.io/posts/2022-09-18-on-reading-academic-papers/","summary":"Continuous learning is a critical skill. The internet houses a panoply of learning material for almost any topic under the sun. YouTube is my go-to for learning anything and everything - for it is easy to digest and entertaining. I\u0026rsquo;d regard YouTube to be on the lower side of information density. Contrastingly, reading acadmic papers is something I\u0026rsquo;ve begun recently. YouTube and academic papers are on the opposite extremes of information density - which is why I like reading papers.","title":"On reading academic papers"},{"content":"It was for a project during my undergrad that I grew interest for this strange programming language. It was suggested by none other than my team mate, and hardcore Rustacean Devdutt.\nRust is different from the languages I\u0026rsquo;ve been used to, like Python and JavaScript. I have tried my hands at static typed low level languages like C and C++ and in my opinion, they are easy to get started with. But Rust is not. Rust has a steep learning curve and it takes time.\nI feel that the tradeoff here is that with C/C++, though it\u0026rsquo;s easier to get started, it takes a whole lot of time and effort to write memory safe and efficient programs. But with Rust, safety and efficiency are built-in from day one. Along with Rust\u0026rsquo;s ownership concept and the compiler that enforces it, you will get used to the underlying rules. Moreover, the compiler is patient enough to explain what went wrong, and where.\nCommunity Rust is a fairly new programming language. It doesn\u0026rsquo;t have polished edges and sometimes you get stuck while learning stuff. For a beginner, a good community is a boon. Thankfully Rust has a great community.\nAt the very beginning you have the most excellent Rust tutorial - The Rust Book. Well explained and goes in detail about the most useful parts of the language. The second best thing is the people who share what they\u0026rsquo;ve learnt along their journey - through blogs and youtube videos. (Yes, those 2hr long streaming sessions on youtube. Those too)\nLow level concepts Learn Rust - that\u0026rsquo;s the advice I\u0026rsquo;d give my younger self when starting my undergrad in CS. Rust is a low level language but still retains a high level syntax. The concepts remain, and there is a lot you\u0026rsquo;ll get to learn. As a CS student, it is extremely important to understand the low level working of a program and concepts like memory management, concurrency and the like.\nYou can build systems software projects like CLIs, operating systems, game engines, embedded system software or even web servers. My first systems software prorject was a simple shell with a few builtins. It was written in C. See vsh. I learnt about how a shell works, and how inputs and outputs are processed.\nAnother one is timers which is written in Rust and has better program design. One of the main problems I\u0026rsquo;ve faced with this is the time drift due to task switch from the CPU. The initial implementation relied on manually keeping track of each second passed by incrementing a counter.\nI\u0026rsquo;ve been learning about async rust and that itself has led me down a rabbit hole about how a runtime and the underlying operating system manages memory. I got exposed to hands on experience with Linux system calls and such. More on that in another post. Rust has certainly opened me up to a lot of low level concepts during my journey. Combined with my point above on community, the learning part is fun.\nType System Coming from a dynamic programming language like Python, facing the type system was a challenge. I had to keep track of what types my data had, and when I passed it around in my code. But now after working on a few projects, I\u0026rsquo;ve come to like it. It provides more control over how I use and pass memory. It also gives a clear structure to the data representations. The Rust compiler has been very helpful by pointing out errors and possible solutions.\nCompiler The compiler is helpful and strict at the same time. I should admit that the compiler is a bit slow but that is bearable when compared to the advantages it provides. Apart from the safety promises and performance optimizations, the error messages are articulate and the solutions are straight forward. There is a lot to learn just from the compiler itself.\nThis is my experience exploring the language till now. It has been a fun journey and I plan to continue down this lane. I wouldn\u0026rsquo;t have given this a shot if it wasn\u0026rsquo;t for my hardcore fellow Rustacean Devdutt. Hopefully, you\u0026rsquo;ll get to hear more of this!\n","permalink":"https://123vivekr.github.io/posts/2022-02-08-rust-first_impressions/","summary":"It was for a project during my undergrad that I grew interest for this strange programming language. It was suggested by none other than my team mate, and hardcore Rustacean Devdutt.\nRust is different from the languages I\u0026rsquo;ve been used to, like Python and JavaScript. I have tried my hands at static typed low level languages like C and C++ and in my opinion, they are easy to get started with.","title":"Rust: First Impressions"},{"content":"This post is a summary of the work that has been completed during the GSoC 2020 period for my project, Object Tracking. The project consisted of implementing an Object Tracking UI in Pitivi and the associated tracking functionality in GStreamer.\ncvtracker GStreamer element flatpak: add opencv_contrib\nI began by adding the opencv_contrib module to the opencv installation for the Pitivi development environment. opencv_contrib contains the library with the tracking algorithms.\nOpenCV Tracker Element for Object Tracking\nThis merge request introduces the tracking functionality in the gst-plugins-bad package.\nPreviously the opencv plugin was not being built due to the unavailability of the headers. This patch fixes the problem, ensuring that the headers are detected for the correct version of opencv and the plugin is built.\nIn the next commit: opencv: add cvtracker plugin, I implement the cvtracker element as part of the opencv GStreamer plugin and set up the associated tests to ensure correct working. The next commit: meson: add opencv/tracking header requirement ensures that the tracking library is available before building the plugin.\nI implemented an additional feature to draw a rectangle over the tracked object. This will come in handy during the testing phase and for live tracking in Pitivi. opencv: cvtracker: add draw property\nA brief explanation of the cvtracker element can be found on my blog post: cvtracker: OpenCV object tracking plugin\nTracker Perspective With the GStreamer element for tracking completed, the next stage was to implement a user-friendly UI for object tracking. The new Tracker Perspective replaces Pitivi\u0026rsquo;s main window, allowing to select an object on the viewer and track it. If the chosen tracking algorithm fails to track the object correctly, the tracking can be redone from a corrected position.\nThe work done on the Pitivi side currently resides in the Merge Request\nA brief explanation of tracking objects can be found on my blog post:\nPitivi: Object Tracking\nAs the project progressed, we iterated and made a lot of UI improvements. A demo with the explanation can be found here: Pitivi: Edit Object Tracking\nScreenshot of the TrackerPerspective UI Selecting an object from the viewer Live tracking in TrackerPerspective Processing the tracked objects `Cover Object` button to add effects and track objects Cover Object Popover Cover effect on clip A tracked object can be covered with a colored rectangle on a clip in the timeline. This can be done easily through the \u0026ldquo;Cover Object\u0026rdquo; button in the \u0026ldquo;Clip Properties\u0026rdquo; middle pane shown when a clip is selected.\nWhen an Asset from the Media Library is dragged and dropped on the timeline, it becomes a Clip. A user can create multiple Clips from a single Asset.\nThe tracked objects belong to the Asset. That means if an object is tracked, it’s available to all the Clips backed by the particular Asset. The tracked data is then applied to the properties of an Effect applied to the Clip to obtain the ‘cover rectangle’. Clips can have object effects independent of each other.\nIf a tracked object is deleted from the Assed in the TrackerPerspective, the effects associated with that object will be deleted from all the Clips of that Asset. The below figure shows the situation when ‘Object 2’ is deleted.\nTechnical Note: Taking advantage of Assets being MetaContainers, we store the Objects\u0026rsquo;s tracking data as a pitivi::tracker_data metadata item. The tracking data is saved in the Project\u0026rsquo;s .xges file by GES when the Project is saved.\nProject Status The tracker element and Pitivi UI are complete, as demonstrated in my blog posts. However, there are 2 major bugs in the feature for adding effects to the tracked objects.\nThe first bug is when the user adds the effect for two objects, one with more tracking data than the other, the video track of the clip gets disfigured. This might be due to no available tracking data before the start, which causes the tracking box (red box in this case) to be shown at the bottom-right corner until it receives its first tracking data point. Here’s a small demo of the bug: YouTube\nThe fix for this bug is to add a zorder property to the gescompositor element of the effect and to set the tracking data point to some random far away point before the beginning of the actual tracked data points.\nThe second bug is when the user adds the effect for an object and then resizes the clip in the viewer, the effect doesn’t follow. Here’s a demo for the bug: YouTube\nThe currently discussed solution is to re-adjust the tracking data every time the user makes a change to the clip video track in the viewer, by resetting the ControlSource.\n","permalink":"https://123vivekr.github.io/posts/2020-08-29-pitivi-gsoc-work-product/","summary":"This post is a summary of the work that has been completed during the GSoC 2020 period for my project, Object Tracking. The project consisted of implementing an Object Tracking UI in Pitivi and the associated tracking functionality in GStreamer.\ncvtracker GStreamer element flatpak: add opencv_contrib\nI began by adding the opencv_contrib module to the opencv installation for the Pitivi development environment. opencv_contrib contains the library with the tracking algorithms.","title":"GSoC 2020: A summary"},{"content":"My last post was about adding a feature to track objects. But sometimes the algorithm doesn\u0026rsquo;t track the object 100% correct, so in this post, I present to you a new update which lets the user edit the tracked data easily in the Pitivi Tracker Perspective itself.\nDemo See the feature in action. YouTube\nIn the video, the user selects a clip and goes to the Tracker Perspective, by clicking on the \u0026ldquo;Track Object\u0026rdquo; button. Now, the user selects the object to track and chooses the algorithm before tracking. Pitivi tracks the object for the rest of the clip.\nBut wait, the user has accidentally chosen only a portion of the object. They can correct this by seeking to a point in the tracking and selecting the object again, this time, they get it right :) After the tracking is completed, the tracking data is updated to accomodate the updated tracking co-ordinates.\nSimilarly, we can correct faults in the tracking.\nUI improvements Cursor changes to crosshair when hovering on the viewer Track Object button is inside Blur Object popover Start position of the tracked object shown as a marker on the seekbar Added an infobar to show instructions Info bar disappears on choosing an object Further developments A feature to add an effect to the tracked objects is in the development stage. The tracked objects will be shown in the \u0026ldquo;Blur Object\u0026rdquo; popover. The user can add an effect by clicking on the object. More on that in another post. :)\n","permalink":"https://123vivekr.github.io/posts/2020-08-16-pitivi-object_track_editing/","summary":"My last post was about adding a feature to track objects. But sometimes the algorithm doesn\u0026rsquo;t track the object 100% correct, so in this post, I present to you a new update which lets the user edit the tracked data easily in the Pitivi Tracker Perspective itself.\nDemo See the feature in action. YouTube\nIn the video, the user selects a clip and goes to the Tracker Perspective, by clicking on the \u0026ldquo;Track Object\u0026rdquo; button.","title":"Tracking adjustments in Pitivi"},{"content":"I’ve been selected as a student developer at Pitivi for Google Summer of Code 2020. My project is to create an object tracking and blurring feature.\nThe tracking is done by passing the video clip through a pipeline which includes a tracker plugin. So, the first goal of the project was to implement the tracker plugin in GStreamer.\nIntroducing cvtracker This is a GStreamer plugin which allows the user to select an object in the initial frame of a clip by specifying the object’s bounding box (x, y, width and height coordinates). The element then tracks the object during the subsequent frames of the clip.\nThis plugin is in the gst-plugins-bad module. It is currently a merge request.\nThe plugin can be used by anyone by just installing the module. An example pipeline is given below.\nExample A sample pipeline with cvtracker looks like this:\ngst-launch-1.0 filesrc location=t.mp4 ! decodebin ! videoconvert ! cvtracker object-initial-x=175 object-initial-y=40object-initial-width=300 object-initial-height=150 algorithm=1 ! videoconvert ! xvimagesink Here’s a demo of the pipeline given above: YouTube\nAlgorithm The tracker incorporates OpenCV’s long term tracker cv::Tracker.\nThe available tracking algorithms are:\nBoosting - the Boosting tracker CSRT - the CSRT tracker KCF - the KCF (Kernelized Correlation Filter) tracker MedianFlow - the Median Flow tracker MIL - the MIL tracker MOSSE - the MOSSE (Minimum Output Sum of Squared Error) tracker TLD - the TLD (Tracking, learning and detection) tracker You might wonder why we missed the GOTURN algorithm. It was skipped due to the added complexity of setting up the models by the user.\nProperties algorithm - the tracking algorithm to use draw-rect - to draw a rectangle around the tracked object object-initial-x - object’s initial x coordinate object-initial-x - object’s initial y coordinate object-initial-height - object’s initial height object-initial-width - object’s initial width The element sends out the tracked object’s bounding box’s x, y, width and height coordinates through the pipeline bus and also through the buffer. If you want live tracking during the playback, you could use the draw-rect property.\n","permalink":"https://123vivekr.github.io/posts/2020-07-28-gstreamer-cvtracker/","summary":"I’ve been selected as a student developer at Pitivi for Google Summer of Code 2020. My project is to create an object tracking and blurring feature.\nThe tracking is done by passing the video clip through a pipeline which includes a tracker plugin. So, the first goal of the project was to implement the tracker plugin in GStreamer.\nIntroducing cvtracker This is a GStreamer plugin which allows the user to select an object in the initial frame of a clip by specifying the object’s bounding box (x, y, width and height coordinates).","title":"cvtracker: An object tracking plugin for GStreamer"},{"content":"I’ve been selected as a student developer at Pitivi for Google Summer of Code 2020. My project is to create an object tracking and blurring feature.\nIn this post, I introduce a feature in development which allows the user to track an object inside a video clip.\nObject tracking in action Before diving into the aspects, let’s see it in action. YouTube\nIn the video, the user selects the clip to be used and clicks on the “Track object” button. In the next screen (tracker perspective), the user chooses a frame and selects the object to be tracked using a drag-and-drop motion. The user then sets the tracking algorithm and initiates the tracking. Live tracking is displayed. The tracked object appears on the left pane. The user has the option to delete the tracked object.\nInternals The cvtracker is a plugin from gst-plugins-bad project (which is also a part of my GSoC project). It allows us to track the object by running the clip through a pipeline. The tracking data is available through the bus and buffer metadata.\nThe tracking in pitivi is implemented using a pipeline, which runs the clip and feeds it to the cvtracker. We extract the region-of-interest (ROI) data from the buffer.\nAn Object Manager class stores all the tracked objects in a clip. Technically, the object data is saved to the asset metadata. So every clip that gets generated using the asset has access to all the tracked objects.\nTracking data For receiving the tracking data from the cvtracker, we use fakesink with the properties: fakesink name=sink signal-handoffs=TRUE.\nThen we connect the handoff signal to the callback function:\ndef __tracker_handoff_cb(self, unused_element, buffer, unused_pad, roi_data): video_roi = GstVideo.buffer_get_video_region_of_interest_meta_id(buffer, 0) if video_roi: roi_data[buffer.pts] = (video_roi.x, video_roi.y, video_roi.w, video_roi.h) else: self.log(\u0026#34;lost tracker at: %s\u0026#34; + str(buffer.pts / Gst.SECOND)) Further developments There’s more coming! Sometimes the tracking can be a little inaccurate, so we’re working on a feature to adjust the tracking of an object. Basically the user can manually adjust the tracking data using a simple and user friendly interface, integrated right into the tracker perspective. More on that in another post.\n","permalink":"https://123vivekr.github.io/posts/2020-07-28-pitivi-object_tracking/","summary":"I’ve been selected as a student developer at Pitivi for Google Summer of Code 2020. My project is to create an object tracking and blurring feature.\nIn this post, I introduce a feature in development which allows the user to track an object inside a video clip.\nObject tracking in action Before diving into the aspects, let’s see it in action. YouTube\nIn the video, the user selects the clip to be used and clicks on the “Track object” button.","title":"Pitivi can now track objects"},{"content":"I am Vivek and I build stuff.\nPrimarily a software engineer interested in web, linux, system design and machine learning. Here I\u0026rsquo;ll be documenting my software development journey along with a myraid of other things which I\u0026rsquo;m deeply interested in.\nIf you find my work interesting, do feel free to contact me on one of my social media platforms. I\u0026rsquo;m most active on my twitter.\n","permalink":"https://123vivekr.github.io/about/","summary":"I am Vivek and I build stuff.\nPrimarily a software engineer interested in web, linux, system design and machine learning. Here I\u0026rsquo;ll be documenting my software development journey along with a myraid of other things which I\u0026rsquo;m deeply interested in.\nIf you find my work interesting, do feel free to contact me on one of my social media platforms. I\u0026rsquo;m most active on my twitter.","title":"Welcome to my blog!"}]